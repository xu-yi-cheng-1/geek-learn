<p>你好，我是王喆。今天我们聊聊Embedding。</p><p>说起Embedding，我想你肯定不会陌生，至少经常听说。事实上，Embedding技术不仅名气大，而且用Embedding方法进行相似物品推荐，几乎成了业界最流行的做法，无论是国外的Facebook、Airbnb，还是在国内的阿里、美团，我们都可以看到Embedding的成功应用。因此，自从深度学习流行起来之后，Embedding就成为了深度学习推荐系统方向最火热的话题之一。</p><p>但是Embedding这个词又不是很好理解，你甚至很难给它找出一个准确的中文翻译，如果硬是翻译成“嵌入”“向量映射”，感觉也不知所谓。所以索性我们就还是用Embedding这个叫法吧。</p><p>那这项技术到底是什么，为什么它在推荐系统领域这么重要？最经典的Embedding方法Word2vec的原理细节到底啥样？这节课，我们就一起来聊聊这几个问题。</p><h2>什么是Embedding？</h2><p>简单来说，<strong>Embedding就是用一个数值向量“表示”一个对象（Object）的方法</strong>，我这里说的对象可以是一个词、一个物品，也可以是一部电影等等。但是“表示”这个词是什么意思呢？用一个向量表示一个物品，这句话感觉还是有点让人费解。</p><!-- [[[read_end]]] --><p>这里，我先尝试着解释一下：一个物品能被向量表示，是因为这个向量跟其他物品向量之间的距离反映了这些物品的相似性。更进一步来说，两个向量间的距离向量甚至能够反映它们之间的关系。这个解释听上去可能还是有点抽象，那我们再用两个具体的例子解释一下。</p><p>图1是Google著名的论文Word2vec中的例子，它利用Word2vec这个模型把单词映射到了高维空间中，每个单词在这个高维空间中的位置都非常有意思，你看图1左边的例子，从king到queen的向量和从man到woman的向量，无论从方向还是尺度来说它们都异常接近。这说明什么？这说明词Embedding向量间的运算居然能够揭示词之间的性别关系！比如woman这个词的词向量可以用下面的运算得出：</p><p>Embedding(<strong>woman</strong>)=Embedding(<strong>man</strong>)+[Embedding(<strong>queen</strong>)-Embedding(<strong>king</strong>)]</p><p>同样，图1右的例子也很典型，从walking到walked和从swimming到swam的向量基本一致，这说明词向量揭示了词之间的时态关系！这就是Embedding技术的神奇之处。</p><p><img src="https://static001.geekbang.org/resource/image/19/0a/19245b8bc3ebd987625e36881ca4f50a.jpeg" alt="" title="图1 词向量例子"></p><p>你可能会觉得词向量技术离推荐系统领域还是有一点远，那Netflix应用的电影Embedding向量方法，就是一个非常直接的推荐系统应用。从Netflix利用矩阵分解方法生成的电影和用户的Embedding向量示意图中，我们可以看出不同的电影和用户分布在一个二维的空间内，由于Embedding向量保存了它们之间的相似性关系，因此有了这个Embedding空间之后，我们再进行电影推荐就非常容易了。具体来说就是，我们直接找出某个用户向量周围的电影向量，然后把这些电影推荐给这个用户就可以了。这就是Embedding技术在推荐系统中最直接的应用。</p><p><img src="https://static001.geekbang.org/resource/image/da/4c/da7e73faacc5e6ea1c02345386bf6f4c.jpeg" alt="" title="图2 电影-用户向量例子"></p><h2>Embedding技术对深度学习推荐系统的重要性</h2><p>事实上，我一直把Embedding技术称作深度学习的“基础核心操作”。在推荐系统领域进入深度学习时代之后，Embedding技术更是“如鱼得水”。那为什么Embedding技术对于推荐系统如此重要，Embedding技术又在特征工程中发挥了怎样的作用呢？针对这两个问题，我主要有两点想和你深入聊聊。</p><p><strong>首先，Embedding是处理稀疏特征的利器。</strong> 上节课我们学习了One-hot编码，因为推荐场景中的类别、ID型特征非常多，大量使用One-hot编码会导致样本特征向量极度稀疏，而深度学习的结构特点又不利于稀疏特征向量的处理，因此几乎所有深度学习推荐模型都会由Embedding层负责将稀疏高维特征向量转换成稠密低维特征向量。所以说各类Embedding技术是构建深度学习推荐模型的基础性操作。</p><p><strong>其次，Embedding可以融合大量有价值信息，本身就是极其重要的特征向量 。</strong> 相比由原始信息直接处理得来的特征向量，Embedding的表达能力更强，特别是Graph Embedding技术被提出后，Embedding几乎可以引入任何信息进行编码，使其本身就包含大量有价值的信息，所以通过预训练得到的Embedding向量本身就是极其重要的特征向量。</p><p>因此我们才说，Embedding技术在深度学习推荐系统中占有极其重要的位置，熟悉并掌握各类流行的Embedding方法是构建一个成功的深度学习推荐系统的有力武器。<strong>这两个特点也是我们为什么把Embedding的相关内容放到特征工程篇的原因，因为它不仅是一种处理稀疏特征的方法，也是融合大量基本特征，生成高阶特征向量的有效手段。</strong></p><h2>经典的Embedding方法，Word2vec</h2><p>提到Embedding，就一定要深入讲解一下Word2vec。它不仅让词向量在自然语言处理领域再度流行，更关键的是，自从2013年谷歌提出Word2vec以来，Embedding技术从自然语言处理领域推广到广告、搜索、图像、推荐等几乎所有深度学习的领域，成了深度学习知识框架中不可或缺的技术点。Word2vec作为经典的Embedding方法，熟悉它对于我们理解之后所有的Embedding相关技术和概念都是至关重要的。下面，我就给你详细讲一讲Word2vec的原理。</p><h3>什么是Word2vec？</h3><p>Word2vec是“word to vector”的简称，顾名思义，它是一个生成对“词”的向量表达的模型。</p><p>想要训练Word2vec模型，我们需要准备由一组句子组成的语料库。假设其中一个长度为T的句子包含的词有w<sub>1</sub>,w<sub>2</sub>……w<sub>t</sub>，并且我们假定每个词都跟其相邻词的关系最密切。</p><p>根据模型假设的不同，Word2vec模型分为两种形式，CBOW模型（图3左）和Skip-gram模型（图3右）。其中，CBOW模型假设句子中每个词的选取都由相邻的词决定，因此我们就看到CBOW模型的输入是w<sub>t</sub>周边的词，预测的输出是w<sub>t</sub>。Skip-gram模型则正好相反，它假设句子中的每个词都决定了相邻词的选取，所以你可以看到Skip-gram模型的输入是w<sub>t</sub>，预测的输出是w<sub>t</sub>周边的词。按照一般的经验，Skip-gram模型的效果会更好一些，所以我接下来也会以Skip-gram作为框架，来给你讲讲Word2vec的模型细节。</p><p><img src="https://static001.geekbang.org/resource/image/f2/8a/f28a06f57e4aeb5f826df466cbe6288a.jpeg" alt="" title="图3 Word2vec的两种模型结构CBOW和Skip-gram"></p><h3>Word2vec的样本是怎么生成的？</h3><p>我们先来看看<strong>训练Word2vec的样本是怎么生成的。</strong> 作为一个自然语言处理的模型，训练Word2vec的样本当然来自于语料库，比如我们想训练一个电商网站中关键词的Embedding模型，那么电商网站中所有物品的描述文字就是很好的语料库。</p><p>我们从语料库中抽取一个句子，选取一个长度为2c+1（目标词前后各选c个词）的滑动窗口，将滑动窗口由左至右滑动，每移动一次，窗口中的词组就形成了一个训练样本。根据Skip-gram模型的理念，中心词决定了它的相邻词，我们就可以根据这个训练样本定义出Word2vec模型的输入和输出，输入是样本的中心词，输出是所有的相邻词。</p><p>为了方便你理解，我再举一个例子。这里我们选取了“Embedding技术对深度学习推荐系统的重要性”作为句子样本。首先，我们对它进行分词、去除停用词的过程，生成词序列，再选取大小为3的滑动窗口从头到尾依次滑动生成训练样本，然后我们把中心词当输入，边缘词做输出，就得到了训练Word2vec模型可用的训练样本。</p><p><img src="https://static001.geekbang.org/resource/image/e8/1f/e84e1bd1f7c5950fb70ed63dda0yy21f.jpeg" alt="" title="图4 生成Word2vec训练样本的例子"></p><h3>Word2vec模型的结构是什么样的？</h3><p>有了训练样本之后，我们最关心的当然是Word2vec这个模型的结构是什么样的。我相信，通过第3节课的学习，你已经掌握了神经网络的基础知识，那再理解Word2vec的结构就容易多了，它的结构本质上就是一个三层的神经网络（如图5）。</p><p><img src="https://static001.geekbang.org/resource/image/99/39/9997c61588223af2e8c0b9b2b8e77139.jpeg" alt="" title="图5 Word2vec模型的结构
"></p><p>它的输入层和输出层的维度都是V，这个V其实就是语料库词典的大小。假设语料库一共使用了10000个词，那么V就等于10000。根据图4生成的训练样本，这里的输入向量自然就是由输入词转换而来的One-hot编码向量，输出向量则是由多个输出词转换而来的Multi-hot编码向量，显然，基于Skip-gram框架的Word2vec模型解决的是一个多分类问题。</p><p>隐层的维度是N，N的选择就需要一定的调参能力了，我们需要对模型的效果和模型的复杂度进行权衡，来决定最后N的取值，并且最终每个词的Embedding向量维度也由N来决定。</p><p>最后是激活函数的问题，这里我们需要注意的是，隐层神经元是没有激活函数的，或者说采用了输入即输出的恒等函数作为激活函数，而输出层神经元采用了softmax作为激活函数。</p><p>你可能会问为什么要这样设置Word2vec的神经网络，以及我们为什么要这样选择激活函数呢？因为这个神经网络其实是为了表达从输入向量到输出向量的这样的一个条件概率关系，我们看下面的式子：</p><p>$$p\left(w_{O} \mid w_{I}\right)=\frac{\exp \left(v_{w_{O}}^{\prime}{v}_{w_{I}}\right)}{\sum_{i=1}^{V} \exp \left(v_{w_{i}}^{\prime}{ }^{\top} v_{w_{I}}\right)}$$</p><p>这个由输入词WI预测输出词WO的条件概率，其实就是Word2vec神经网络要表达的东西。我们通过极大似然的方法去最大化这个条件概率，就能够让相似的词的内积距离更接近，这就是我们希望Word2vec神经网络学到的。</p><p>当然，如果你对数学和机器学习的底层理论没那么感兴趣的话，也不用太深入了解这个公式的由来，因为现在大多数深度学习平台都把它们封装好了，你不需要去实现损失函数、梯度下降的细节，你只要大概清楚他们的概念就可以了。</p><p>如果你是一个理论派，其实Word2vec还有很多值得挖掘的东西，比如，为了节约训练时间，Word2vec经常会采用负采样（Negative Sampling）或者分层softmax（Hierarchical Softmax）的训练方法。关于这一点，我推荐你去阅读<a href="https://github.com/wzhe06/Reco-papers/blob/master/Embedding/%5BWord2Vec%5D%20Word2vec%20Parameter%20Learning%20Explained%20%28UMich%202016%29.pdf">《Word2vec Parameter Learning Explained》</a>这篇文章，相信你会找到最详细和准确的解释。</p><h3>怎样把词向量从Word2vec模型中提取出来？</h3><p>在训练完Word2vec的神经网络之后，可能你还会有疑问，我们不是想得到每个词对应的Embedding向量嘛，这个Embedding在哪呢？其实，它就藏在输入层到隐层的权重矩阵WVxN中。我想看了下面的图你一下就明白了。</p><p><img src="https://static001.geekbang.org/resource/image/0d/72/0de188f4b564de8076cf13ba6ff87872.jpeg" alt="" title="图6 词向量藏在Word2vec的权重矩阵中"></p><p>你可以看到，输入向量矩阵WVxN的每一个行向量对应的就是我们要找的“词向量”。比如我们要找词典里第i个词对应的Embedding，因为输入向量是采用One-hot编码的，所以输入向量的第i维就应该是1，那么输入向量矩阵WVxN中第i行的行向量自然就是该词的Embedding啦。</p><p>细心的你可能也发现了，输出向量矩阵$W'$也遵循这个道理，确实是这样的，但一般来说，我们还是习惯于使用输入向量矩阵作为词向量矩阵。</p><p>在实际的使用过程中，我们往往会把输入向量矩阵转换成词向量查找表（Lookup table，如图7所示）。例如，输入向量是10000个词组成的One-hot向量，隐层维度是300维，那么输入层到隐层的权重矩阵为10000x300维。在转换为词向量Lookup table后，每行的权重即成了对应词的Embedding向量。如果我们把这个查找表存储到线上的数据库中，就可以轻松地在推荐物品的过程中使用Embedding去计算相似性等重要的特征了。</p><p><img src="https://static001.geekbang.org/resource/image/1e/96/1e6b464b25210c76a665fd4c34800c96.jpeg" alt="" title="图7 Word2vec的Lookup table"></p><h3>Word2vec对Embedding技术的奠基性意义</h3><p>Word2vec是由谷歌于2013年正式提出的，其实它并不完全是原创性的，学术界对词向量的研究可以追溯到2003年，甚至更早的时期。但正是谷歌对Word2vec的成功应用，让词向量的技术得以在业界迅速推广，进而使Embedding这一研究话题成为热点。毫不夸张地说，Word2vec对深度学习时代Embedding方向的研究具有奠基性的意义。</p><p>从另一个角度来看，Word2vec的研究中提出的模型结构、目标函数、负采样方法、负采样中的目标函数在后续的研究中被重复使用并被屡次优化。掌握Word2vec中的每一个细节成了研究Embedding的基础。从这个意义上讲，熟练掌握本节课的内容是非常重要的。</p><h2>Item2Vec：Word2vec方法的推广</h2><p>在Word2vec诞生之后，Embedding的思想迅速从自然语言处理领域扩散到几乎所有机器学习领域，推荐系统也不例外。既然Word2vec可以对词“序列”中的词进行Embedding，那么对于用户购买“序列”中的一个商品，用户观看“序列”中的一个电影，也应该存在相应的Embedding方法。</p><p><img src="https://static001.geekbang.org/resource/image/d8/07/d8e3cd26a9ded7e79776dd31cc8f4807.jpeg" alt="" title="图8 不同场景下的序列数据"></p><p>于是，微软于2015年提出了Item2Vec方法，它是对Word2vec方法的推广，使Embedding方法适用于几乎所有的序列数据。Item2Vec模型的技术细节几乎和Word2vec完全一致，只要能够用序列数据的形式把我们要表达的对象表示出来，再把序列数据“喂”给Word2vec模型，我们就能够得到任意物品的Embedding了。</p><p>Item2vec的提出对于推荐系统来说当然是至关重要的，因为它使得“万物皆Embedding”成为了可能。对于推荐系统来说，Item2vec可以利用物品的Embedding直接求得它们的相似性，或者作为重要的特征输入推荐模型进行训练，这些都有助于提升推荐系统的效果。</p><h2>小结</h2><p>这节课，我们一起学习了深度学习推荐系统中非常重要的知识点，Embedding。Embedding就是用一个数值向量“表示”一个对象的方法。通过Embedding，我们又引出了Word2vec，Word2vec是生成对“词”的向量表达的模型。其中，Word2vec的训练样本是通过滑动窗口一一截取词组生成的。在训练完成后，模型输入向量矩阵的行向量，就是我们要提取的词向量。最后，我们还学习了Item2vec，它是Word2vec在任意序列数据上的推广。</p><p>我把这些重点的内容以表格的形式，总结了出来，方便你随时回顾。</p><p><img src="https://static001.geekbang.org/resource/image/0f/7b/0f0f9ffefa0c610dd691b51c251b567b.jpeg" alt=""></p><p>这节课，我们主要对序列数据进行了Embedding化，那如果是图结构的数据怎么办呢？另外，有没有什么好用的工具能实现Embedding技术呢？接下来的两节课，我就会一一讲解图结构数据的Embedding方法Graph Embedding，并基于Spark对它们进行实现。</p><h2>课后思考</h2><p>在我们通过Word2vec训练得到词向量，或者通过Item2vec得到物品向量之后，我们应该用什么方法计算他们的相似性呢？你知道几种计算相似性的方法？</p><p>如果你身边的朋友正对Embedding技术感到疑惑，也欢迎你把这节课分享给TA，我们下节课再见！</p>